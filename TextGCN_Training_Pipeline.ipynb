{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-GCN Training Pipeline\n",
    "\n",
    "This notebook walks through the complete Text-GCN training pipeline for text classification.\n",
    "\n",
    "## Overview\n",
    "Text-GCN builds a heterogeneous graph containing:\n",
    "- **Document nodes**: Each text document is a node\n",
    "- **Word nodes**: Each word in the vocabulary is a node\n",
    "- **Edges**: Connect documents to words (TF-IDF) and words to words (PMI)\n",
    "\n",
    "Then applies Graph Convolutional Networks for classification.\n",
    "\n",
    "## Pipeline Steps\n",
    "1. Configuration & Setup\n",
    "2. Data Loading & Graph Construction\n",
    "3. Model Creation\n",
    "4. Training Loop\n",
    "5. Evaluation\n",
    "6. Results Analysis\n",
    "\n",
    "## Memory Management\n",
    "âš ï¸ This notebook includes explicit memory cleanup (`del` and `gc.collect()`) to handle large datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Add project root to path if needed\n",
    "# project_root = os.path.dirname(os.path.abspath('__file__'))\n",
    "# sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from config import FLAGS\n",
    "from load_data import load_data\n",
    "from saver import Saver\n",
    "from model_factory import create_model\n",
    "from eval import eval, MovingAverage\n",
    "\n",
    "print(\"âœ“ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display configuration\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT-GCN CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: {FLAGS.dataset}\")\n",
    "print(f\"Device: {FLAGS.device}\")\n",
    "print(f\"Multi-label: {FLAGS.multi_label}\")\n",
    "print(f\"Validation metric: {FLAGS.validation_metric}\")\n",
    "print(f\"\\nTraining Parameters:\")\n",
    "print(f\"  - Epochs: {FLAGS.num_epochs}\")\n",
    "print(f\"  - Learning rate: {FLAGS.lr}\")\n",
    "print(f\"  - Window size: {FLAGS.word_window_size}\")\n",
    "print(f\"  - Min word frequency: {FLAGS.min_word_freq}\")\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  - Model: {FLAGS.model}\")\n",
    "print(f\"  - Node embedding: {FLAGS.node_embd}\")\n",
    "print(f\"  - Layer dims: {FLAGS.layer_dims}\")\n",
    "print(f\"  - Dropout: {FLAGS.dropout}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Initialize Saver (for logging and model checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[1/5] Initializing saver...\")\n",
    "saver = Saver()\n",
    "print(f\"  âœ“ Logging to: {saver.logdir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading & Graph Construction\n",
    "\n",
    "This step:\n",
    "1. Loads cleaned text documents and labels\n",
    "2. Builds vocabulary (filtered by min_word_freq)\n",
    "3. Constructs the heterogeneous graph:\n",
    "   - Document-word edges (TF-IDF weights)\n",
    "   - Word-word edges (PMI weights from co-occurrence)\n",
    "4. Splits data into train/val/test sets\n",
    "\n",
    "**Note**: This can take 5-15 minutes for large datasets (cached after first run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[2/5] Loading data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_data, val_data, test_data, raw_doc_list = load_data()\n",
    "\n",
    "print(f\"\\n  âœ“ Data loaded in {time.time() - start_time:.1f}s\")\n",
    "print(f\"  âœ“ Graph shape: {train_data.graph.shape}\")\n",
    "print(f\"  âœ“ Train samples: {len(train_data.node_ids)}\")\n",
    "print(f\"  âœ“ Val samples: {len(val_data.node_ids)}\")\n",
    "print(f\"  âœ“ Test samples: {len(test_data.node_ids)}\")\n",
    "print(f\"  âœ“ Vocabulary size: {len(train_data.vocab)}\")\n",
    "print(f\"  âœ“ Total nodes (docs + words): {train_data.graph.shape[0]}\")\n",
    "print(f\"  âœ“ Total edges: {train_data.graph.nnz}\")\n",
    "\n",
    "# Memory cleanup: raw_doc_list not needed after loading\n",
    "del raw_doc_list, start_time\n",
    "gc.collect()\n",
    "print(\"  âœ“ Memory cleanup: Removed raw_doc_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect sample documents\n",
    "print(\"\\nSample documents (first 3):\")\n",
    "for i, doc_id in enumerate(train_data.node_ids[:3]):\n",
    "    label = train_data.label_inds[doc_id]\n",
    "    print(f\"\\nDoc {i+1} (node {doc_id}):\")\n",
    "    print(f\"  Label: {label}\")\n",
    "    print(f\"  Text: {train_data.docs_dict[doc_id][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distribution\n",
    "if FLAGS.multi_label:\n",
    "    # Multi-label classification\n",
    "    train_labels = np.array([train_data.label_inds[i] for i in train_data.node_ids])\n",
    "    label_counts = train_labels.sum(axis=0)\n",
    "    print(\"\\nLabel distribution (multi-label):\")\n",
    "    for i, count in enumerate(label_counts):\n",
    "        print(f\"  Label {i}: {int(count)} ({100*count/len(train_data.node_ids):.1f}%)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del train_labels, label_counts\n",
    "else:\n",
    "    # Single-label classification\n",
    "    train_labels = [train_data.label_inds[i] for i in train_data.node_ids]\n",
    "    unique, counts = np.unique(train_labels, return_counts=True)\n",
    "    print(\"\\nLabel distribution (single-label):\")\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"  Class {label}: {count} ({100*count/len(train_labels):.1f}%)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del train_labels, unique, counts\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Creation\n",
    "\n",
    "Initialize node features and create the Text-GCN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[3/5] Creating model...\")\n",
    "print(\"  â†’ Initializing node features...\")\n",
    "\n",
    "# Initialize node features (one-hot encoding)\n",
    "train_data.init_node_feats(FLAGS.init_type, FLAGS.device)\n",
    "val_data.init_node_feats(FLAGS.init_type, FLAGS.device)\n",
    "test_data.init_node_feats(FLAGS.init_type, FLAGS.device)\n",
    "\n",
    "print(f\"  âœ“ Feature initialization: {FLAGS.init_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"  â†’ Building model architecture...\")\n",
    "model = create_model(train_data)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n  âœ“ Model created\")\n",
    "print(f\"  âœ“ Total parameters: {total_params:,}\")\n",
    "print(f\"  âœ“ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\n  Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Cleanup\n",
    "del total_params, trainable_params\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training Loop\n",
    "\n",
    "Train the model for the specified number of epochs with validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[4/5] Training model...\")\n",
    "print(f\"  â†’ Setting up training (lr={FLAGS.lr}, epochs={FLAGS.num_epochs})...\")\n",
    "\n",
    "# Setup training\n",
    "moving_avg = MovingAverage(FLAGS.validation_window_size, FLAGS.validation_metric != 'loss')\n",
    "pyg_graph = train_data.get_pyg_graph(FLAGS.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=FLAGS.lr)\n",
    "\n",
    "print(f\"\\n  {'='*56}\")\n",
    "print(f\"  Starting training for {FLAGS.num_epochs} epochs\")\n",
    "print(f\"  {'='*56}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "training_history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_metric': [],\n",
    "    'epoch_times': []\n",
    "}\n",
    "\n",
    "for epoch in range(FLAGS.num_epochs):\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Training step\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1:03d}: Forward pass...\", end='', flush=True)\n",
    "    t_forward_start = time.time()\n",
    "    loss, preds_train = model(pyg_graph, train_data)\n",
    "    t_forward = time.time() - t_forward_start\n",
    "    print(f\" âœ“ ({t_forward:.2f}s)\", flush=True)\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1:03d}: Backward pass...\", end='', flush=True)\n",
    "    t_backward_start = time.time()\n",
    "    loss.backward()\n",
    "    t_backward = time.time() - t_backward_start\n",
    "    print(f\" âœ“ ({t_backward:.2f}s)\", flush=True)\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1:03d}: Optimizer step...\", end='', flush=True)\n",
    "    t_opt_start = time.time()\n",
    "    optimizer.step()\n",
    "    t_opt = time.time() - t_opt_start\n",
    "    print(f\" âœ“ ({t_opt:.2f}s)\", flush=True)\n",
    "    \n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    # Clean up training predictions (not needed)\n",
    "    del preds_train\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    # Validation step\n",
    "    print(f\"  Epoch {epoch+1:03d}: Validation...\", end='', flush=True)\n",
    "    t_val_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        val_loss, preds_val = model(pyg_graph, val_data)\n",
    "        val_loss = val_loss.item()\n",
    "        eval_res_val = eval(preds_val, val_data)\n",
    "    t_val = time.time() - t_val_start\n",
    "    print(f\" âœ“ ({t_val:.2f}s)\", flush=True)\n",
    "    \n",
    "    # Clean up validation predictions\n",
    "    del preds_val\n",
    "    \n",
    "    # Get validation metric\n",
    "    metric_val = eval_res_val[FLAGS.validation_metric]\n",
    "    is_best = len(moving_avg.results) == 0 or moving_avg.best_result(metric_val)\n",
    "    best_marker = \" ðŸŒŸ NEW BEST\" if is_best else \"\"\n",
    "    \n",
    "    total_time = time.time() - t_start\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  Epoch {epoch+1:03d} Summary: \"\n",
    "          f\"Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "          f\"{FLAGS.validation_metric}={metric_val:.4f}, \"\n",
    "          f\"Total={total_time:.2f}s \"\n",
    "          f\"(fwd:{t_forward:.1f}s + bwd:{t_backward:.1f}s + opt:{t_opt:.1f}s + val:{t_val:.1f}s)\"\n",
    "          f\"{best_marker}\")\n",
    "    \n",
    "    # Save history\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    training_history['val_metric'].append(metric_val)\n",
    "    training_history['epoch_times'].append(total_time)\n",
    "    \n",
    "    # Save best model\n",
    "    if is_best:\n",
    "        saver.save_trained_model(model, epoch + 1)\n",
    "        print(f\"    â†’ Model saved at epoch {epoch+1}\")\n",
    "    \n",
    "    moving_avg.add_to_moving_avg(metric_val)\n",
    "    \n",
    "    # Clean up eval results and timing variables\n",
    "    del eval_res_val, t_forward, t_backward, t_opt, t_val, t_start\n",
    "    \n",
    "    # Periodic cleanup every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        print(f\"    â†’ Memory cleanup at epoch {epoch+1}\")\n",
    "\n",
    "print(f\"\\n  {'='*56}\")\n",
    "print(\"  Training complete!\")\n",
    "print(f\"  {'='*56}\")\n",
    "\n",
    "# Load best model\n",
    "print(\"  â†’ Loading best model...\")\n",
    "best_model = saver.load_trained_model(train_data)\n",
    "model = best_model\n",
    "\n",
    "# Clean up optimizer and moving average (not needed anymore)\n",
    "del optimizer, moving_avg, best_model\n",
    "gc.collect()\n",
    "print(\"  âœ“ Memory cleanup: Removed optimizer and training objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Plot losses\n",
    "axes[0].plot(training_history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(training_history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation metric\n",
    "axes[1].plot(training_history['val_metric'], label=FLAGS.validation_metric, marker='o', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel(FLAGS.validation_metric)\n",
    "axes[1].set_title(f'Validation {FLAGS.validation_metric}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining statistics:\")\n",
    "print(f\"  Average epoch time: {np.mean(training_history['epoch_times']):.2f}s\")\n",
    "print(f\"  Best {FLAGS.validation_metric}: {max(training_history['val_metric']):.4f}\")\n",
    "print(f\"  Final train loss: {training_history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final val loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Training history kept for potential future analysis\n",
    "# If memory is critical, uncomment:\n",
    "# del training_history\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Test Set Evaluation\n",
    "\n",
    "Evaluate the best model on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n[5/5] Evaluating on test set...\")\n",
    "with torch.no_grad():\n",
    "    test_loss, preds_test = model(pyg_graph, test_data)\n",
    "\n",
    "print(\"  â†’ Computing final test metrics...\")\n",
    "eval_res = eval(preds_test, test_data, True)\n",
    "y_true = eval_res.pop('y_true')\n",
    "y_pred = eval_res.pop('y_pred')\n",
    "\n",
    "# Clean up test loss (not needed)\n",
    "del test_loss\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "pprint(eval_res)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: preds_test, y_true, y_pred kept for analysis below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics in a formatted table\n",
    "import pandas as pd\n",
    "\n",
    "metrics_df = pd.DataFrame([eval_res]).T\n",
    "metrics_df.columns = ['Value']\n",
    "metrics_df.index.name = 'Metric'\n",
    "print(\"\\nTest Metrics Summary:\")\n",
    "print(metrics_df.to_string())\n",
    "\n",
    "# Clean up\n",
    "del metrics_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multi-label: show per-label performance\n",
    "if FLAGS.multi_label:\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    # Assuming 6 labels for Jigsaw\n",
    "    label_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    print(\"\\nPer-Label Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_names, zero_division=0))\n",
    "else:\n",
    "    # Single-label confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Clean up\n",
    "    del cm, disp\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves (for multi-label classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.multi_label:\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from scipy.special import expit\n",
    "    \n",
    "    # Get probabilities (sigmoid applied to logits)\n",
    "    y_probs = expit(preds_test)  # Apply sigmoid\n",
    "    \n",
    "    label_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    auc_scores = []\n",
    "    for i, label in enumerate(label_names):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores.append(roc_auc)\n",
    "        plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves by Label')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Mean AUC\n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    print(f\"\\nMean AUC across all labels: {mean_auc:.4f}\")\n",
    "    \n",
    "    # Clean up intermediate variables\n",
    "    del fpr, tpr, roc_auc, auc_scores, mean_auc\n",
    "    # Keep y_probs for inference section\n",
    "else:\n",
    "    y_probs = None  # Not applicable for single-label\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Inference on New Data (Optional)\n",
    "\n",
    "Example of how to use the trained model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Get predictions for specific test samples\n",
    "def predict_sample(sample_idx=0):\n",
    "    \"\"\"Get prediction for a specific sample\"\"\"\n",
    "    # Get the document node ID\n",
    "    doc_node_id = test_data.node_ids[sample_idx]\n",
    "    \n",
    "    # Get prediction\n",
    "    if FLAGS.multi_label:\n",
    "        pred_probs_sample = y_probs[sample_idx]  # Already computed\n",
    "        pred_labels = (pred_probs_sample > 0.5).astype(int)\n",
    "        true_labels = test_data.label_inds[doc_node_id]\n",
    "        \n",
    "        label_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "        \n",
    "        print(f\"\\nSample {sample_idx}:\")\n",
    "        print(f\"Text: {test_data.docs_dict[doc_node_id][:200]}...\")\n",
    "        print(f\"\\nPredictions:\")\n",
    "        for i, label in enumerate(label_names):\n",
    "            print(f\"  {label:15s}: Prob={pred_probs_sample[i]:.3f}, Pred={pred_labels[i]}, True={int(true_labels[i])}\")\n",
    "    else:\n",
    "        pred_class = y_pred[sample_idx]\n",
    "        true_class = test_data.label_inds[doc_node_id]\n",
    "        pred_probs_sample = torch.softmax(torch.tensor(preds_test[sample_idx]), dim=0).numpy()\n",
    "        \n",
    "        print(f\"\\nSample {sample_idx}:\")\n",
    "        print(f\"Text: {test_data.docs_dict[doc_node_id][:200]}...\")\n",
    "        print(f\"\\nPredicted class: {pred_class}\")\n",
    "        print(f\"True class: {true_class}\")\n",
    "        print(f\"Class probabilities: {pred_probs_sample}\")\n",
    "\n",
    "# Test on first 3 samples\n",
    "for i in range(min(3, len(test_data.node_ids))):\n",
    "    predict_sample(i)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up predictions after inference examples\n",
    "del preds_test\n",
    "if y_probs is not None:\n",
    "    del y_probs\n",
    "gc.collect()\n",
    "print(\"âœ“ Cleaned up prediction tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model Insights\n",
    "\n",
    "Analyze what the model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most important words (highest degree in graph)\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Get word nodes (after document nodes)\n",
    "num_docs = len(train_data.docs_dict)\n",
    "word_start_idx = num_docs\n",
    "\n",
    "# Calculate degree for word nodes\n",
    "degrees = np.array(train_data.graph.sum(axis=0)).flatten()\n",
    "word_degrees = degrees[word_start_idx:]\n",
    "\n",
    "# Get top words by degree (most connected)\n",
    "top_k = 20\n",
    "top_indices = np.argsort(word_degrees)[-top_k:][::-1]\n",
    "top_words = [train_data.vocab[i] for i in top_indices]\n",
    "top_degrees = word_degrees[top_indices]\n",
    "\n",
    "print(f\"\\nTop {top_k} most connected words in the graph:\")\n",
    "for i, (word, degree) in enumerate(zip(top_words, top_degrees), 1):\n",
    "    print(f\"{i:2d}. {word:20s} (degree: {int(degree):5d})\")\n",
    "\n",
    "# Clean up\n",
    "del top_indices, top_degrees\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of node degrees\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "doc_degrees = degrees[:num_docs]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(doc_degrees, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Document Node Degree Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(word_degrees, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Word Node Degree Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGraph statistics:\")\n",
    "print(f\"  Document nodes: {num_docs}\")\n",
    "print(f\"  Word nodes: {len(train_data.vocab)}\")\n",
    "print(f\"  Total edges: {train_data.graph.nnz}\")\n",
    "print(f\"  Avg document degree: {doc_degrees.mean():.1f}\")\n",
    "print(f\"  Avg word degree: {word_degrees.mean():.1f}\")\n",
    "\n",
    "# Clean up degree arrays\n",
    "del degrees, doc_degrees, word_degrees, num_docs, word_start_idx\n",
    "gc.collect()\n",
    "print(\"\\nâœ“ Cleaned up degree analysis arrays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Cleanup (Optional)\n",
    "\n",
    "If you're done with analysis and want to free up maximum memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up all major objects\n",
    "# del model, train_data, val_data, test_data, pyg_graph\n",
    "# del y_true, y_pred, eval_res, top_words\n",
    "# if 'training_history' in locals():\n",
    "#     del training_history\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "# print(\"âœ“ All major objects cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. âœ… Configuration and setup\n",
    "2. âœ… Data loading and graph construction\n",
    "3. âœ… Model creation\n",
    "4. âœ… Training loop with validation\n",
    "5. âœ… Test set evaluation\n",
    "6. âœ… Results visualization\n",
    "7. âœ… Model insights\n",
    "8. âœ… **Memory management throughout**\n",
    "\n",
    "### Key Takeaways:\n",
    "- Text-GCN builds a heterogeneous graph of documents and words\n",
    "- Graph edges capture semantic relationships (TF-IDF and PMI)\n",
    "- GCN layers propagate information through the graph\n",
    "- The model can handle both single-label and multi-label classification\n",
    "- **Explicit memory cleanup enables handling large datasets**\n",
    "\n",
    "### Memory Management Strategy:\n",
    "- âœ… Delete intermediate variables after use (`del` + `gc.collect()`)\n",
    "- âœ… Clean up predictions after each phase\n",
    "- âœ… Periodic cleanup during training loop\n",
    "- âœ… Clear GPU cache when applicable\n",
    "- âœ… Keep only essential data for subsequent cells\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different hyperparameters (window size, min_word_freq, layer dims)\n",
    "- Try different datasets\n",
    "- Compare with baseline methods (BERT, Logistic Regression, etc.)\n",
    "- Generate Kaggle submission (for Jigsaw dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
